{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/Bert-Transformers/blob/main/ch02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd",
      "metadata": {
        "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
        "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25aa40e3-5109-433f-9153-f5770531fe94",
      "metadata": {
        "id": "25aa40e3-5109-433f-9153-f5770531fe94"
      },
      "source": [
        "# Chapter 2: Working with Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf",
      "metadata": {
        "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf"
      },
      "source": [
        "Packages that are being used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
      "metadata": {
        "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
        "outputId": "e2a33c1f-f59c-4ae8-953a-eb8690247660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.5.1\n",
            "tiktoken version: 0.7.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a",
      "metadata": {
        "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a"
      },
      "source": [
        "- This chapter covers data preparation and sampling to get input data \"ready\" for the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5",
      "metadata": {
        "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2417139b-2357-44d2-bd67-23f5d7f52ae7",
      "metadata": {
        "id": "2417139b-2357-44d2-bd67-23f5d7f52ae7"
      },
      "source": [
        "## 2.1 Understanding word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6816ae-e927-43a9-b4dd-e47a9b0e1cf6",
      "metadata": {
        "id": "0b6816ae-e927-43a9-b4dd-e47a9b0e1cf6"
      },
      "source": [
        "- No code in this section"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f69dab7-a433-427a-9e5b-b981062d6296",
      "metadata": {
        "id": "4f69dab7-a433-427a-9e5b-b981062d6296"
      },
      "source": [
        "- There are many forms of embeddings; we focus on text embeddings in this book"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba08d16f-f237-4166-bf89-0e9fe703e7b4",
      "metadata": {
        "id": "ba08d16f-f237-4166-bf89-0e9fe703e7b4"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/02.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "288c4faf-b93a-4616-9276-7a4aa4b5e9ba",
      "metadata": {
        "id": "288c4faf-b93a-4616-9276-7a4aa4b5e9ba"
      },
      "source": [
        "- LLMs work with embeddings in high-dimensional spaces (i.e., thousands of dimensions)\n",
        "- Since we can't visualize such high-dimensional spaces (we humans think in 1, 2, or 3 dimensions), the figure below illustrates a 2-dimensional embedding space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b80160-1f10-4aad-a85e-9c79444de9e6",
      "metadata": {
        "id": "d6b80160-1f10-4aad-a85e-9c79444de9e6"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/03.webp\" width=\"300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3",
      "metadata": {
        "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3"
      },
      "source": [
        "## 2.2 Tokenizing text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9c90731-7dc9-4cd3-8c4a-488e33b48e80",
      "metadata": {
        "id": "f9c90731-7dc9-4cd3-8c4a-488e33b48e80"
      },
      "source": [
        "- In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b",
      "metadata": {
        "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/04.webp\" width=\"300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cceaa18-833d-46b6-b211-b20c53902805",
      "metadata": {
        "id": "8cceaa18-833d-46b6-b211-b20c53902805"
      },
      "source": [
        "- Load raw text we want to work with\n",
        "- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) is a public domain short story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f9d9b1-6d32-485a-825a-a95392a86d79",
      "metadata": {
        "id": "40f9d9b1-6d32-485a-825a-a95392a86d79"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56488f2c-a2b8-49f1-aaeb-461faad08dce",
      "metadata": {
        "id": "56488f2c-a2b8-49f1-aaeb-461faad08dce"
      },
      "source": [
        "- (If you encounter an `ssl.SSLCertVerificationError` when executing the previous code cell, it might be due to using an outdated Python version; you can find [more information here on GitHub](https://github.com/rasbt/LLMs-from-scratch/pull/403))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a769e87-470a-48b9-8bdb-12841b416198",
      "metadata": {
        "id": "8a769e87-470a-48b9-8bdb-12841b416198",
        "outputId": "9787e090-b521-410f-801a-d89fc4416613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e",
      "metadata": {
        "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e"
      },
      "source": [
        "- The goal is to tokenize and embed this text for an LLM\n",
        "- Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above\n",
        "- The following regular expression will split on whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7",
      "metadata": {
        "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7",
        "outputId": "9d21f6c0-142f-48ee-c07d-ee4eb962de8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sYUmrTmaWBfb"
      },
      "id": "sYUmrTmaWBfb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explaining the Code for Text Tokenization\n",
        "This code snippet demonstrates a basic approach to tokenizing text.\n",
        "\n",
        "Tokenization is the process of breaking down a sequence of text into smaller units called tokens.\n",
        "\n",
        "These tokens can be words, subwords, or even individual characters.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here's a breakdown of the code:\n",
        "\n",
        "    import re\n",
        "\n",
        "This line imports the re module, which provides support for regular expressions in Python.\n",
        "\n",
        "Regular expressions are powerful tools for pattern matching within strings.\n",
        "\n",
        "    text = \"Hello, world. This, is a test.\"\n",
        "\n",
        "This line creates a string variable called text and assigns it a sample sentence.\n",
        "\n",
        "    result = re.split(r'(\\s)', text)\n",
        "\n",
        "This is the core of the tokenization in this specific example.\n",
        "\n",
        "##re.split()\n",
        "is a function from the re module that splits a string by occurrences of a pattern.\n",
        "\n",
        "##r'(\\s)'\n",
        " is the regular expression pattern.\n",
        "\n",
        "##r'':\n",
        "This part indicates a raw string literal in Python. U\n",
        "\n",
        "sing a raw string is common when working with regular expressions because it tells Python to treat backslashes (\\) literally, rather than as escape characters.\n",
        "\n",
        "In many regular expressions, backslashes are used to define special character sequences (like \\s), and using a raw string prevents potential conflicts where Python might try to interpret the backslash itself before the regular expression engine gets to it.\n",
        "\n",
        "##(...):\n",
        "The parentheses create a capturing group.\n",
        "\n",
        "In the context of re.split(), enclosing a part of the pattern in parentheses tells the function to include the matched text for that group in the resulting list of splits, in addition to using it as a delimiter.\n",
        "\n",
        "##\\s:\n",
        "This is a special character sequence (a metacharacter) within the regular expression.\n",
        "\n",
        "It specifically matches any whitespace character. This includes:\n",
        "\n",
        "    Space ()\n",
        "    Tab (\\t)\n",
        "    Newline (\\n)\n",
        "    Carriage return (\\r)\n",
        "    Form feed (\\f)\n",
        "    Vertical tab (\\v)\n",
        "\n",
        "So, when re.split() is given the pattern r'(\\s)' and a string, it will:\n",
        "\n",
        "Find every instance of a whitespace character (\\s) in the string.\n",
        "\n",
        "Use each found whitespace character as a point to split the string.\n",
        "\n",
        "Because the \\s is inside a capturing group (), the whitespace character itself will be included as a separate item in the list of results.\n",
        "\n",
        "This is why, in the earlier example re.split(r'(\\s)', \"Hello, world.\"), the output included the spaces as separate elements in the list.\n",
        "\n",
        "If the parentheses were omitted (re.split(r'\\s', \"Hello, world.\")), the spaces would still be used as delimiters, but they would not appear in the resulting list.\n",
        "\n",
        "A regular expression, often shortened to regex or regexp, is a sequence of characters that defines a search pattern.\n",
        "\n",
        "These patterns are primarily used for string matching and manipulation, including operations like:\n",
        "\n",
        "##Searching:\n",
        "Finding specific patterns within a larger string.\n",
        "\n",
        "##Replacing:\n",
        "Substituting parts of a string that match a pattern with something else.\n",
        "\n",
        "##Validating:\n",
        "Checking if a string conforms to a specific format (like an email address or phone number).\n",
        "\n",
        "----------------------\n",
        "##Parsing:\n",
        "Extracting structured data from unstructured text.\n",
        "\n",
        "Think of a regular expression as a mini-language for describing patterns in text. It uses a combination of literal characters and special characters (called metacharacters) to create flexible and powerful patterns.\n",
        "\n",
        "\n",
        "For example:\n",
        "\n",
        "The regular expression hello would match the exact string \"hello\".\n",
        "\n",
        "The regular expression h.llo would match \"hello\", \"hallo\", \"hxllo\", etc., because the . metacharacter matches any single character (except a newline).\n",
        "\n",
        "The regular expression \\d+ would match one or more digits (\\d matches a digit, and + matches one or more of the preceding element).\n",
        "\n",
        "This could match \"1\", \"123\", \"98765\", and so on.\n",
        "\n",
        "Regular expressions are a fundamental tool in text processing and are supported by many programming languages (like Python, Java, JavaScript, etc.) and text editors.\n",
        "\n",
        "They can be incredibly powerful for complex pattern matching, but they can also be quite concise and sometimes difficult to read for beginners.\n",
        "\n",
        "##Regular expressions are based on formal language theory and computability theory.\n",
        "\n",
        "They are a method for defining patterns in text based on a set of rules and syntax, not on statistical properties or probabilities of character sequences.\n",
        "\n",
        "Here's why they are not statistical:\n",
        "\n",
        "##Deterministic Pattern Matching:\n",
        "A regular expression precisely defines a pattern.\n",
        "\n",
        "A given string either matches the pattern or it doesn't.\n",
        "\n",
        "There's no probability involved in the match itself.\n",
        "\n",
        "##Rule-Based:\n",
        "The behavior of a regular expression is determined by the specific characters and metacharacters used in the pattern, following defined rules (e.g., . matches any single character, * matches zero or more of the preceding element).\n",
        "\n",
        "##No Learning from Data (in their core):\n",
        "\n",
        "The definition of a regular expression is static unless you explicitly modify the pattern string itself. They don't learn patterns from a corpus of text in the way that statistical models do.\n",
        "\n",
        "In contrast, statistical methods for text analysis often involve:\n",
        "\n",
        "##Probabilistic Models:\n",
        "Analyzing the likelihood of certain words or sequences appearing based on observed frequencies in a dataset (e.g., N-gram models).\n",
        "\n",
        "##Machine Learning:\n",
        "Training models on large datasets to identify patterns, which often involves statistical inference and optimization.\n",
        "\n",
        "##Variability and Generalization:\n",
        "\n",
        "Statistical models can often handle variations and generalize to unseen data based on learned distributions, which is not the core function of regular expressions.\n",
        "\n",
        "While regular expressions can be used in conjunction with statistical methods (e.g., using regex to extract features that are then used in a statistical model), the regular expression language itself is a non-statistical method for pattern matching.\n",
        "\n",
        "---------------\n",
        "##r''\n",
        "indicates a raw string, which is often used for regular expressions to avoid issues with backslashes.\n",
        "\n",
        "##\\s\n",
        "is a special sequence that matches any whitespace character (space, tab, newline, etc.).\n",
        "\n",
        "The parentheses () around \\s are important.\n",
        "\n",
        "They tell re.split() to include the matched whitespace characters in the resulting list.\n",
        "\n",
        "Without the parentheses, the whitespace characters would be used as delimiters but not included in the output.\n",
        "\n",
        "The text variable is the string that will be split.\n",
        "\n",
        "The result of this operation will be a list of strings, where the original text is split at each whitespace character, and the whitespace characters themselves are included as separate items in the list.\n",
        "\n",
        "    print(result)\n",
        "\n",
        "This line prints the result list to the console.\n",
        "\n",
        "This will show you the list of tokens generated by splitting the original text based on whitespace."
      ],
      "metadata": {
        "id": "Y4veEXcvRJPr"
      },
      "id": "Y4veEXcvRJPr"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EmaxBXBiRBlc"
      },
      "id": "EmaxBXBiRBlc"
    },
    {
      "source": [
        "print(result)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zqD-YKtCQ6bs"
      },
      "id": "zqD-YKtCQ6bs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a8c40c18-a9d5-4703-bf71-8261dbcc5ee3",
      "metadata": {
        "id": "a8c40c18-a9d5-4703-bf71-8261dbcc5ee3"
      },
      "source": [
        "- We don't only want to split on whitespaces but also commas and periods, so let's modify the regular expression to do that as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea02489d-01f9-4247-b7dd-a0d63f62ef07",
      "metadata": {
        "id": "ea02489d-01f9-4247-b7dd-a0d63f62ef07",
        "outputId": "ea399503-5428-47ca-e7a5-e3dc326f877c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461d0c86-e3af-4f87-8fae-594a9ca9b6ad",
      "metadata": {
        "id": "461d0c86-e3af-4f87-8fae-594a9ca9b6ad"
      },
      "source": [
        "- As we can see, this creates empty strings, let's remove them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d8a6fb7-2e62-4a12-ad06-ccb04f25fed7",
      "metadata": {
        "id": "4d8a6fb7-2e62-4a12-ad06-ccb04f25fed7",
        "outputId": "1fa2a883-07d1-4e08-dafd-353470fe2107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "# Strip whitespace from each item and then filter out any empty strings.\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250e8694-181e-496f-895d-7cb7d92b5562",
      "metadata": {
        "id": "250e8694-181e-496f-895d-7cb7d92b5562"
      },
      "source": [
        "- This looks pretty good, but let's also handle other types of punctuation, such as periods, question marks, and so on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed3a9467-04b4-49d9-96c5-b8042bcf8374",
      "metadata": {
        "id": "ed3a9467-04b4-49d9-96c5-b8042bcf8374",
        "outputId": "c44c891b-6eae-4ab6-b56e-1199bac3ac9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bbea70b-c030-45d9-b09d-4318164c0bb4",
      "metadata": {
        "id": "5bbea70b-c030-45d9-b09d-4318164c0bb4"
      },
      "source": [
        "- This is pretty good, and we are now ready to apply this tokenization to the raw text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a",
      "metadata": {
        "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/05.webp\" width=\"350px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c567caa-8ff5-49a8-a5cc-d365b0a78a99",
      "metadata": {
        "id": "8c567caa-8ff5-49a8-a5cc-d365b0a78a99",
        "outputId": "a07c2dad-32b5-4ca7-f27e-714cabe2e197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complex Regular Expression\n",
        "This line uses the re.split() function again, but with a more complex regular expression pattern: r'([,.:;?_!\"()\\']|--|\\s)'.\n",
        "\n",
        "r'': Again, this indicates a raw string.\n",
        "\n",
        "\n",
        "(...): This is a capturing group.\n",
        "\n",
        "It means that whatever is matched by the pattern inside the parentheses will be included in the resulting list.\n",
        "\n",
        "[,.:;?_!\"()\\']: This part of the pattern is enclosed in square brackets [].\n",
        "\n",
        "In a regular expression, square brackets define a character set. This character set matches any single character that is inside the brackets.\n",
        "\n",
        "So, this part will match a comma (,), a period (.), a colon (:), a semicolon (;), a question mark (?), an underscore (_), an exclamation mark (!), a double quote (\"), an opening parenthesis ((), a closing parenthesis ()), or a single quote (').\n",
        "\n",
        "|: This is the OR operator in regular expressions. It means \"match the pattern on the left OR the pattern on the right\".\n",
        "\n",
        "--: This matches the literal two characters --.\n",
        "\n",
        "|: Another OR operator.\n",
        "\n",
        "\\s: This matches any whitespace character, as we discussed before.\n",
        "\n",
        "So, the entire pattern ([,.:;?_!\"()\\']|--|\\s) tells re.split() to split the raw_text string whenever it encounters any of the following:\n",
        "\n",
        "##Any single character from the set ,.:;?_!\"()\\'.\n",
        "The literal string --.\n",
        "\n",
        "Any whitespace character.\n",
        "\n",
        "Because the entire pattern is within a capturing group (), the delimiters (the punctuation, --, or whitespace) will also be included as separate items in the preprocessed list.\n",
        "\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "This line uses a list comprehension to refine the preprocessed list. It does two things:\n",
        "\n",
        "##for item in preprocessed:\n",
        "It iterates through each item in the list generated by the re.split() call.\n",
        "\n",
        "##if item.strip():\n",
        "This is a filter. The item.strip() method removes leading and trailing whitespace from the item.\n",
        "\n",
        "If, after stripping whitespace, the item is not an empty string (meaning it contained more than just whitespace or was a non-whitespace character/string), the condition if item.strip() evaluates to True.\n",
        "\n",
        "Items that consist only of whitespace (or are empty strings) will evaluate to False and be filtered out.\n",
        "\n",
        "##item.strip():\n",
        "For the items that pass the filter, item.strip() is performed, and the resulting string (with leading/trailing whitespace removed) is included in the new preprocessed list.\n",
        "\n",
        "This step is important because the re.split() with a capturing group can sometimes produce empty strings in the output list, especially when multiple delimiters appear consecutively.\n",
        "\n",
        "The if item.strip() part effectively removes these empty strings.\n",
        "\n",
        "    print(preprocessed[:30])\n",
        "\n",
        "Finally, this line prints the first 30 items of the preprocessed list.\n",
        "\n",
        "This gives you a peek at the beginning of the tokenized text to see how the splitting and filtering worked."
      ],
      "metadata": {
        "id": "ccIagwlmX2I2"
      },
      "id": "ccIagwlmX2I2"
    },
    {
      "cell_type": "markdown",
      "id": "e2a19e1a-5105-4ddb-812a-b7d3117eab95",
      "metadata": {
        "id": "e2a19e1a-5105-4ddb-812a-b7d3117eab95"
      },
      "source": [
        "- Let's calculate the total number of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35db7b5e-510b-4c45-995f-f5ad64a8e19c",
      "metadata": {
        "id": "35db7b5e-510b-4c45-995f-f5ad64a8e19c",
        "outputId": "c5684652-9dcc-4165-d78c-3edcb28d6b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4690\n"
          ]
        }
      ],
      "source": [
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231",
      "metadata": {
        "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231"
      },
      "source": [
        "## 2.3 Converting tokens into token IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff",
      "metadata": {
        "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff"
      },
      "source": [
        "- Next, we convert the text tokens into token IDs that we can process via embedding layers later"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d",
      "metadata": {
        "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5973794-7002-4202-8b12-0900cd779720",
      "metadata": {
        "id": "b5973794-7002-4202-8b12-0900cd779720"
      },
      "source": [
        "- From these tokens, we can now build a vocabulary that consists of all the unique tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
      "metadata": {
        "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
        "outputId": "1e078b63-c77e-49a1-c0e8-f338e16c784f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1130\n"
          ]
        }
      ],
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Tokens to Token IDs\n",
        "This section of the code focuses on preparing the tokenized text for use in a large language model (LLM) by converting the text tokens into numerical identifiers, or token IDs. LLMs typically process numerical data, not raw text.\n",
        "\n",
        "---------------\n",
        "--------------\n",
        "First, let's look at the code block you provided:\n",
        "\n",
        "    all_words = sorted(set(preprocessed))\n",
        "    vocab_size = len(all_words)\n",
        "\n",
        "    print(vocab_size)\n",
        "\n",
        "----------------\n",
        "#Building a Vocabulary\n",
        "\n",
        "The goal of this code is to create a vocabulary from the preprocessed list.\n",
        "\n",
        "Remember that the preprocessed list contains all the individual tokens (words and punctuation) extracted from the text.\n",
        "\n",
        "##set(preprocessed):\n",
        "This part of the code takes the preprocessed list and converts it into a set.\n",
        "\n",
        "A set is a collection of unique items. By converting the list to a set, we automatically remove any duplicate tokens, leaving only the unique words and punctuation marks present in the text.\n",
        "\n",
        "\n",
        "##This collection of unique tokens forms our vocabulary.\n",
        "\n",
        "##sorted(...):\n",
        "The sorted() function is then used to sort the unique tokens alphabetically.\n",
        "\n",
        "While sorting isn't strictly necessary for building a vocabulary, it can be helpful for consistency and organization.\n",
        "\n",
        "##all_words = ...:\n",
        "The result of the sorted set of unique tokens is assigned to the variable all_words.\n",
        "\n",
        "This variable now holds a sorted list of every unique token found in the original text.\n",
        "\n",
        "##vocab_size = len(all_words):\n",
        "The len() function is used to get the number of items in the all_words list.\n",
        "\n",
        "This count represents the total number of unique tokens in our vocabulary, which is referred to as the vocab_size.\n",
        "\n",
        "##print(vocab_size):\n",
        "\n",
        "Finally, this line prints the calculated vocab_size to the console.\n",
        "\n",
        "After executing this code, the vocab_size variable will contain the number of distinct tokens identified in the text, which is an important value for subsequent steps like creating an embedding layer for the LLM."
      ],
      "metadata": {
        "id": "aQqhjrg9ZicP"
      },
      "id": "aQqhjrg9ZicP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77d00d96-881f-4691-bb03-84fec2a75a26",
      "metadata": {
        "id": "77d00d96-881f-4691-bb03-84fec2a75a26"
      },
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Vocabulary Dictionary\n",
        "This line of code creates a Python dictionary called vocab.\n",
        "\n",
        "----------\n",
        "----------------\n",
        "##Dictionaries are useful for storing key-value pairs.\n",
        "\n",
        "    vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "\n",
        "---\n",
        "Let's break down what this code does:\n",
        "\n",
        "##all_words:\n",
        "This variable is assumed to be a list containing all the unique tokens from the text you processed earlier.\n",
        "\n",
        "The elements in this list are strings (the tokens).\n",
        "\n",
        "##enumerate(all_words):\n",
        "The enumerate() function is a built-in Python function that takes an iterable (like our all_words list) and returns an iterator that produces pairs of index and value.\n",
        "\n",
        "So, for each token in all_words, enumerate will provide a number (starting from 0) and the token itself.\n",
        "\n",
        "For example, if all_words is ['.', ',', 'Hello', 'world'], enumerate will yield (0, '.'), then (1, ','), then (2, 'Hello'), and so on.\n",
        "\n",
        "##for integer, token in enumerate(all_words):\n",
        " is the loop part of a dictionary comprehension.\n",
        "\n",
        " It iterates through the pairs generated by enumerate(all_words).\n",
        "\n",
        " In each iteration, the index from enumerate is assigned to the variable integer, and the token is assigned to the variable token.\n",
        "\n",
        "##token:integer:\n",
        "This is the key-value pair that will be added to the dictionary for each iteration.\n",
        "\n",
        "The token (the string representing the unique word or punctuation) becomes the key, and the integer (the numerical index assigned by enumerate) becomes the value.\n",
        "\n",
        "##{...}:\n",
        "The curly braces indicate that we are creating a dictionary using a dictionary comprehension.\n",
        "\n",
        "-----------------\n",
        "#In summary,\n",
        "this code iterates through the sorted list of unique tokens (all_words) and assigns a unique integer ID to each token.\n",
        "\n",
        "The resulting vocab dictionary maps each token (string) to its corresponding integer ID.\n",
        "\n",
        "This is a crucial step in preparing text data for machine learning models, as models typically work with numerical representations rather than raw text.\n",
        "\n",
        "This dictionary will be used later to convert sequences of tokens into sequences of integer IDs, which can then be processed by embedding layers."
      ],
      "metadata": {
        "id": "imG4ZtjLbbX4"
      },
      "id": "imG4ZtjLbbX4"
    },
    {
      "cell_type": "markdown",
      "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3",
      "metadata": {
        "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3"
      },
      "source": [
        "- Below are the first 50 entries in this vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6",
      "metadata": {
        "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6",
        "outputId": "0a6a0acf-1acf-46db-b707-cda9f48c1af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19",
      "metadata": {
        "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19"
      },
      "source": [
        "- Below, we illustrate the tokenization of a short sample text using a small vocabulary:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc",
      "metadata": {
        "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a",
      "metadata": {
        "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a"
      },
      "source": [
        "- Putting it now all together into a tokenizer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5",
      "metadata": {
        "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "source": [
        "jjclass SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YCLinO5dXg6T"
      },
      "id": "YCLinO5dXg6T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimpleTokenizerV1 Class\n",
        "This code defines a Python class called SimpleTokenizerV1.\n",
        "\n",
        "This class is a custom tokenizer designed to convert text into a sequence of numerical IDs (encoding) and convert those IDs back into text (decoding).\n",
        "\n",
        "It uses a predefined vocabulary to perform these conversions.\n",
        "\n",
        "    __init__ Method\n",
        "    class SimpleTokenizerV1:\n",
        "        def __init__(self, vocab):\n",
        "          self.str_to_int = vocab\n",
        "          self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "##The __init__ method\n",
        "is the constructor for the SimpleTokenizerV1 class.\n",
        "\n",
        "It's called when you create a new instance of the class.\n",
        "\n",
        "##self:\n",
        "This refers to the instance of the class being created.\n",
        "\n",
        "##vocab:\n",
        "This is an input parameter that is expected to be a dictionary.\n",
        "\n",
        "This dictionary represents the vocabulary, where keys are the text tokens (strings) and values are their corresponding integer IDs.\n",
        "\n",
        "\n",
        "##Inside the __init__ method:\n",
        "\n",
        "##self.str_to_int = vocab:\n",
        "This line stores the input vocab dictionary in an instance variable called str_to_int.\n",
        "\n",
        "This dictionary will be used to look up the integer ID for a given text token during the encoding process.\n",
        "\n",
        "##self.int_to_str = {i:s for s,i in vocab.items()}:\n",
        "This line creates a new dictionary called int_to_str and stores it as an instance variable.\n",
        "\n",
        "This dictionary is the inverse of the vocab dictionary.\n",
        "\n",
        "It's created using a dictionary comprehension that iterates through the items (key-value pairs) of the vocab dictionary.\n",
        "\n",
        "For each item (s, i), where s is the string token and i is the integer ID, it creates a new key-value pair i:s in the self.int_to_str dictionary.\n",
        "\n",
        "This dictionary will be used to look up the text token for a given integer ID during the decoding process.\n",
        "\n",
        "-------------------\n",
        "##In essence,\n",
        "the __init__ method initializes the tokenizer with a vocabulary and creates two dictionaries: one to map strings to integers and another to map integers back to strings.\n",
        "\n",
        "##encode Method\n",
        "    def encode(self, text):\n",
        "          preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "                                \n",
        "          preprocessed = [\n",
        "              item.strip() for item in preprocessed if item.strip()\n",
        "           ]\n",
        "           ids = [self.str_to_int[s] for s in preprocessed]\n",
        "           return ids\n",
        "\n",
        "The encode method takes a string of text as input and converts it into a list of integer IDs based on the tokenizer's vocabulary.\n",
        "\n",
        "##self:\n",
        "Refers to the instance of the class.\n",
        "\n",
        "##text:\n",
        "The input string to be encoded.\n",
        "Inside the encode method:\n",
        "\n",
        "##preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text):\n",
        "This line uses the re.split() function from the re module (which is assumed to be imported) to split the input text into a list of tokens.\n",
        "\n",
        "The regular expression pattern r'([,.:;?_!\"()\\']|--|\\s)' is used as the delimiter.\n",
        "\n",
        "This pattern splits the text based on various punctuation marks (comma, period, colon, etc.), double hyphens (--), or whitespace characters (\\s).\n",
        "\n",
        "The parentheses around the pattern create a capturing group, which means the delimiters themselves are also included in the resulting preprocessed list.\n",
        "\n",
        "##preprocessed = [item.strip() for item in preprocessed if item.strip()]:\n",
        "\n",
        "This is a list comprehension that cleans up the preprocessed list.\n",
        "\n",
        "It iterates through each item in the list. item.strip() removes leading and trailing whitespace from the item.\n",
        "\n",
        "The if item.strip() part acts as a filter, including the item in the new list only if, after stripping whitespace, the item is not an empty string.\n",
        "\n",
        "This removes any empty strings that might have resulted from the splitting process.\n",
        "\n",
        "##ids = [self.str_to_int[s] for s in preprocessed]:\n",
        "This is another list comprehension. It iterates through the cleaned preprocessed list of text tokens (s).\n",
        "\n",
        "For each token s, it looks up its corresponding integer ID in the self.str_to_int dictionary (the vocabulary mapping strings to integers) and adds that ID to a new list called ids.\n",
        "\n",
        "##return ids:\n",
        "The method returns the final list of integer IDs, which is the encoded representation of the input text.\n",
        "decode Method\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "The decode method takes a list of integer IDs as input and converts it back into a text string using the tokenizer's vocabulary.\n",
        "\n",
        "##self:\n",
        "Refers to the instance of the class.\n",
        "ids: The input list of integer IDs to be decoded.\n",
        "\n",
        "\n",
        "Inside the decode method:\n",
        "\n",
        "##text = \" \".join([self.int_to_str[i] for i in ids]):\n",
        "\n",
        "This line converts the list of integer IDs back into a string. It uses a list comprehension to iterate through each integer ID (i) in the ids list.\n",
        "\n",
        "For each ID i, it looks up the corresponding text token in the self.int_to_str dictionary (the vocabulary mapping integers back to strings).\n",
        "\n",
        "The \"\".join(...) part then concatenates these text tokens into a single string, using a space character \" \" as the separator between tokens.\n",
        "\n",
        "This initially reconstructs the text with spaces between every token, including spaces before punctuation.\n",
        "\n",
        "##text = re.sub(r'\\s+([,.?!\"()\\'])', ##r'\\1', text):\n",
        "This line uses the re.sub() function to clean up the spacing around punctuation.\n",
        "\n",
        "##r'\\s+([,.?!\"()\\'])':\n",
        "This is the regular expression pattern to search for.\n",
        "\n",
        "##\\s+:\n",
        "Matches one or more whitespace characters.\n",
        "\n",
        "##([,.?!\"()\\']):\n",
        "This is a capturing group that matches any single character from the set: comma, period, question mark, exclamation mark, double quote, opening parenthesis, or closing parenthesis.\n",
        "\n",
        "##r'\\1':\n",
        "This is the replacement string. \\1 refers to the content of the first capturing group in the pattern.\n",
        "\n",
        "The re.sub() function finds all occurrences of the pattern (one or more spaces followed by one of the specified punctuation marks) and replaces them with just the punctuation mark itself (captured in group 1).\n",
        "\n",
        "This effectively removes the spaces before the punctuation marks that were introduced by the \"\".join() operation.\n",
        "\n",
        "##return text:\n",
        "The method returns the reconstructed and cleaned text string.\n",
        "\n",
        "-----\n",
        "##In summary,\n",
        "the SimpleTokenizerV1 class provides basic text tokenization functionality using a predefined vocabulary, allowing for the conversion of text to numerical IDs and back."
      ],
      "metadata": {
        "id": "o5-strobfUAa"
      },
      "id": "o5-strobfUAa"
    },
    {
      "cell_type": "markdown",
      "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7",
      "metadata": {
        "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7"
      },
      "source": [
        "- The `encode` function turns text into token IDs\n",
        "- The `decode` function turns token IDs back into text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc21d347-ec03-4823-b3d4-9d686e495617",
      "metadata": {
        "id": "cc21d347-ec03-4823-b3d4-9d686e495617"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/08.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c",
      "metadata": {
        "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c"
      },
      "source": [
        "- We can use the tokenizer to encode (that is, tokenize) texts into integers\n",
        "- These integers can then be embedded (later) as input of/for the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
      "metadata": {
        "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
        "outputId": "2658f01c-1473-4d79-bb62-e646c8e7c476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3201706e-a487-4b60-b99d-5765865f29a0",
      "metadata": {
        "id": "3201706e-a487-4b60-b99d-5765865f29a0"
      },
      "source": [
        "- We can decode the integers back into text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d8c8fb-432d-4a49-b332-99f23b233746",
      "metadata": {
        "id": "01d8c8fb-432d-4a49-b332-99f23b233746",
        "outputId": "ee191932-82ab-41ed-a49b-81dd1bd44d6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54f6aa8b-9827-412e-9035-e827296ab0fe",
      "metadata": {
        "id": "54f6aa8b-9827-412e-9035-e827296ab0fe",
        "outputId": "97436958-4acf-4cfd-e8b2-89e19d480a7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b821ef8-4d53-43b6-a2b2-aef808c343c7",
      "metadata": {
        "id": "4b821ef8-4d53-43b6-a2b2-aef808c343c7"
      },
      "source": [
        "## 2.4 Adding special context tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "863d6d15-a3e2-44e0-b384-bb37f17cf443",
      "metadata": {
        "id": "863d6d15-a3e2-44e0-b384-bb37f17cf443"
      },
      "source": [
        "- It's useful to add some \"special\" tokens for unknown words and to denote the end of a text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa7fc96c-e1fd-44fb-b7f5-229d7c7922a4",
      "metadata": {
        "id": "aa7fc96c-e1fd-44fb-b7f5-229d7c7922a4"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/09.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d709d57-2486-4152-b7f9-d3e4bd8634cd",
      "metadata": {
        "id": "9d709d57-2486-4152-b7f9-d3e4bd8634cd"
      },
      "source": [
        "- Some tokenizers use special tokens to help the LLM with additional context\n",
        "- Some of these special tokens are\n",
        "  - `[BOS]` (beginning of sequence) marks the beginning of text\n",
        "  - `[EOS]` (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n",
        "  - `[PAD]` (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n",
        "- `[UNK]` to represent words that are not included in the vocabulary\n",
        "\n",
        "- Note that GPT-2 does not need any of these tokens mentioned above but only uses an `<|endoftext|>` token to reduce complexity\n",
        "- The `<|endoftext|>` is analogous to the `[EOS]` token mentioned above\n",
        "- GPT also uses the `<|endoftext|>` for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n",
        "- GPT-2 does not use an `<UNK>` token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a336b43b-7173-49e7-bd80-527ad4efb271",
      "metadata": {
        "id": "a336b43b-7173-49e7-bd80-527ad4efb271"
      },
      "source": [
        "- We use the `<|endoftext|>` tokens between two independent sources of text:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52442951-752c-4855-9752-b121a17fef55",
      "metadata": {
        "id": "52442951-752c-4855-9752-b121a17fef55"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c661a397-da06-4a86-ac27-072dbe7cb172",
      "metadata": {
        "id": "c661a397-da06-4a86-ac27-072dbe7cb172"
      },
      "source": [
        "- Let's see what happens if we tokenize the following text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5767eff-440c-4de1-9289-f789349d6b85",
      "metadata": {
        "id": "d5767eff-440c-4de1-9289-f789349d6b85",
        "outputId": "4233f655-42c1-456a-f2d9-cc90f6bc52b3"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'Hello'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SimpleTokenizerV1(vocab)\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea. Is this-- a test?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n",
            "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
            "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"Hello, do you like tea. Is this-- a test?\"\n",
        "\n",
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc53ee0c-fe2b-4cd8-a946-5471f7651acf",
      "metadata": {
        "id": "dc53ee0c-fe2b-4cd8-a946-5471f7651acf"
      },
      "source": [
        "- The above produces an error because the word \"Hello\" is not contained in the vocabulary\n",
        "- To deal with such cases, we can add special tokens like `\"<|unk|>\"` to the vocabulary to represent unknown words\n",
        "- Since we are already extending the vocabulary, let's add another token called `\"<|endoftext|>\"` which is used in GPT-2 training to denote the end of a text (and it's also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f",
      "metadata": {
        "id": "ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f"
      },
      "outputs": [],
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c3143b-e860-4d3b-a22a-de22b547a6a9",
      "metadata": {
        "id": "57c3143b-e860-4d3b-a22a-de22b547a6a9",
        "outputId": "b8556380-d17b-4fe2-88fe-45e1cf6bd596"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e51bb1-ae05-4aa8-a9ff-455b65ed1959",
      "metadata": {
        "id": "50e51bb1-ae05-4aa8-a9ff-455b65ed1959",
        "outputId": "c2158080-f100-4ef6-cd91-2ac987d4de4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1daa2b0-6e75-412b-ab53-1f6fb7b4d453",
      "metadata": {
        "id": "a1daa2b0-6e75-412b-ab53-1f6fb7b4d453"
      },
      "source": [
        "- We also need to adjust the tokenizer accordingly so that it knows when and how to use the new `<unk>` token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948861c5-3f30-4712-a234-725f20d26f68",
      "metadata": {
        "id": "948861c5-3f30-4712-a234-725f20d26f68"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimpleTokenizerV2 Class\n",
        "This code defines a Python class named SimpleTokenizerV2.\n",
        "\n",
        "This class is designed to handle the process of converting text into a sequence of numerical IDs (encoding) and converting those IDs back into text (decoding).\n",
        "\n",
        "It builds upon the functionality of SimpleTokenizerV1 by adding support for an \"unknown\" token (<|unk|>) to handle words that are not present in its vocabulary.\n",
        "\n",
        "    class SimpleTokenizerV2:\n",
        "       def __init__(self, vocab):\n",
        "           self.str_to_int = vocab\n",
        "           self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "---------------------------------------\n",
        "##__init__ Method\n",
        "The __init__ method is the constructor for the SimpleTokenizerV2 class. It is called when you create a new instance of the class.\n",
        "\n",
        "##self:\n",
        "This refers to the instance of the class being created.\n",
        "\n",
        "##vocab:\n",
        "This is an input parameter that is expected to be a dictionary.\n",
        "\n",
        "This dictionary represents the vocabulary, where keys are the text tokens (strings) and values are their corresponding integer IDs.\n",
        "\n",
        "--------------------\n",
        "#Inside the __init__ method:\n",
        "\n",
        "##self.str_to_int = vocab:\n",
        "This line stores the input vocab dictionary in an instance variable called str_to_int.\n",
        "\n",
        "This dictionary will be used to look up the integer ID for a given text token during the encoding process.\n",
        "\n",
        "----------------------------\n",
        "##self.int_to_str = { i:s for s,i in vocab.items()}:\n",
        "This line creates a new dictionary called int_to_str and stores it as an instance variable.\n",
        "\n",
        "This dictionary is the inverse of the vocab dictionary.\n",
        "\n",
        "It is created using a dictionary comprehension that iterates through the items (key-value pairs) of the vocab dictionary. For each item (s, i), where s is the string token and i is the integer ID, it creates a new key-value pair i:s in the self.int_to_str dictionary.\n",
        "\n",
        "This dictionary will be used to look up the text token for a given integer ID during the decoding process.\n",
        "\n",
        "------\n",
        "##In essence,\n",
        "the __init__ method initializes the tokenizer with a vocabulary and creates two dictionaries: one to map strings to integers and another to map integers back to strings.\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "##encode Method\n",
        "The encode method takes a string of text as input and converts it into a list of integer IDs based on the tokenizer's vocabulary.\n",
        "\n",
        "------\n",
        "\n",
        "##self:\n",
        "Refers to the instance of the class.\n",
        "\n",
        "##text:\n",
        "The input string to be encoded.\n",
        "Inside the encode method:\n",
        "\n",
        "##preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text):\n",
        "This line uses the re.split() function from the re module (which is assumed to be imported) to split the input text into a list of tokens.\n",
        "\n",
        "The regular expression pattern r'([,.:;?_!\"()\\']|--|\\s)' is used as the delimiter.\n",
        "\n",
        "This pattern splits the text based on various punctuation marks (comma, period, colon, etc.), double hyphens (--), or whitespace characters (\\s).\n",
        "\n",
        "The parentheses around the pattern create a capturing group, which means the delimiters themselves are also included in the resulting preprocessed list.\n",
        "\n",
        "##preprocessed = [item.strip() for item in preprocessed if item.strip()]:\n",
        "This is a list comprehension that cleans up the preprocessed list.\n",
        "\n",
        "It iterates through each item in the list. item.strip() removes leading and trailing whitespace from the item.\n",
        "\n",
        "The if item.strip() part acts as a filter, including the item in the new list only if, after stripping whitespace, the item is not an empty string.\n",
        "\n",
        "This removes any empty strings that might have resulted from the splitting process.\n",
        "\n",
        "##preprocessed = [ item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed ]:\n",
        "\n",
        "This is another list comprehension that handles out-of-vocabulary words. It iterates through the cleaned preprocessed list.\n",
        "\n",
        "For each item, it checks if the item exists as a key in the self.str_to_int dictionary (the vocabulary).\n",
        "\n",
        "If the item is found in the vocabulary (item in self.str_to_int is True), the item itself is kept.\n",
        "\n",
        "If the item is not found in the vocabulary, the string \" <|unk|>\" is used instead to represent the unknown token.\n",
        "\n",
        "The result is a new list of tokens, where unknown words are replaced by the <|unk|> token.\n",
        "\n",
        "##ids = [self.str_to_int[s] for s in preprocessed]:\n",
        "This is the final list comprehension in the encoding process.\n",
        "\n",
        "It iterates through the preprocessed list (which now contains either vocabulary tokens or the <|unk|> token).\n",
        "\n",
        "For each token s in this list, it looks up its corresponding integer ID in the self.str_to_int dictionary and adds that ID to a new list called ids.\n",
        "\n",
        "##return ids:\n",
        "The method returns the final list of integer IDs, which is the encoded representation of the input text.\n",
        "\n",
        "-------------------\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "##decode Method\n",
        "The decode method takes a list of integer IDs as input and converts it back into a text string using the tokenizer's vocabulary.\n",
        "\n",
        "self: Refers to the instance of the class.\n",
        "\n",
        "ids: The input list of integer IDs to be decoded.\n",
        "Inside the decode method:\n",
        "\n",
        "##text = \" \".join([self.int_to_str[i] for i in ids]):\n",
        "This line converts the list of integer IDs back into a string.\n",
        "\n",
        "It uses a list comprehension to iterate through each integer ID (i) in the ids list. For each ID i, it looks up the corresponding text token in the self.int_to_str dictionary (the vocabulary mapping integers back to strings).\n",
        "\n",
        "The \" \".join(...) part then concatenates these text tokens into a single string, using a space character \" \" as the separator between tokens.\n",
        "\n",
        "This initially reconstructs the text with spaces between every token, including spaces before punctuation.\n",
        "\n",
        "##text = re.sub(r'\\s+([,.:;?!\"()\\'])##r'\\1', text):\n",
        "\n",
        "\n",
        "This line uses the re.sub() function to clean up the spacing around punctuation.\n",
        "\n",
        "##r'\\s+([,.:;?!\"()\\'])':\n",
        "This is the regular expression pattern to search for. \\s+ matches one or more whitespace characters.\n",
        "\n",
        "##([,.:;?!\"()\\'])\n",
        "It is a capturing group that matches any single character from the specified set of punctuation.\n",
        "\n",
        "##r'\\1':\n",
        "This is the replacement string. \\1 refers to the content of the first capturing group in the pattern (which is the matched punctuation character).\n",
        "\n",
        "##The re.sub() function\n",
        "\n",
        "finds all occurrences of the pattern (one or more spaces followed by one of the specified punctuation marks) and replaces them with just the punctuation mark itself.\n",
        "\n",
        "This effectively removes the spaces before the punctuation marks that were introduced by the \" \".join() operation.\n",
        "\n",
        "##return text:\n",
        "The method returns the reconstructed and cleaned text string.\n",
        "\n",
        "--------\n",
        "##In summary,\n",
        "the SimpleTokenizerV2 class provides basic text tokenization functionality using a predefined vocabulary, including handling unknown tokens, allowing for the conversion of text to numerical IDs and back."
      ],
      "metadata": {
        "id": "udIhUnzfyUfN"
      },
      "id": "udIhUnzfyUfN"
    },
    {
      "cell_type": "markdown",
      "id": "aa728dd1-9d35-4ac7-938f-d411d73083f6",
      "metadata": {
        "id": "aa728dd1-9d35-4ac7-938f-d411d73083f6"
      },
      "source": [
        "Let's try to tokenize text with the modified tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4133c502-18ac-4412-9f43-01caf4efa3dc",
      "metadata": {
        "id": "4133c502-18ac-4412-9f43-01caf4efa3dc",
        "outputId": "ccf2a879-0024-44a0-9fa3-53766a4c17d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ed395fe-dc1b-4ed2-b85b-457cc35aab60",
      "metadata": {
        "id": "7ed395fe-dc1b-4ed2-b85b-457cc35aab60",
        "outputId": "3e349f11-ef51-4c28-e196-e631392a49e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059367f9-7a60-4c0d-8a00-7c4c766d0ebc",
      "metadata": {
        "id": "059367f9-7a60-4c0d-8a00-7c4c766d0ebc",
        "outputId": "378d90a7-feb7-46a0-ea95-4399a467e1dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
      "metadata": {
        "id": "5c4ba34b-170f-4e71-939b-77aabb776f14"
      },
      "source": [
        "## 2.5 BytePair encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2309494c-79cf-4a2d-bc28-a94d602f050e",
      "metadata": {
        "id": "2309494c-79cf-4a2d-bc28-a94d602f050e"
      },
      "source": [
        "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
        "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
        "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
        "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
        "- In this chapter, we are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance\n",
        "- I created a notebook in the [./bytepair_encoder](../02_bonus_bytepair-encoder) that compares these two implementations side-by-side (tiktoken was about 5x faster on the sample text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede1d41f-934b-4bf4-8184-54394a257a94",
      "metadata": {
        "id": "ede1d41f-934b-4bf4-8184-54394a257a94"
      },
      "outputs": [],
      "source": [
        "# pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
      "metadata": {
        "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
        "outputId": "388ad239-b231-401c-bf08-e0b5d96c7b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tiktoken version: 0.7.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tiktoken\n",
        "\n",
        "tiktoken is an open-source library from OpenAI.\n",
        "\n",
        "This means the code for it is publicly available for use and modification.\n",
        "\n",
        "A BytePair Encoding (BPE) tokenizer.\n",
        "\n",
        " This is a specific type of algorithm used to break down text into smaller units (tokens).\n",
        "\n",
        " BPE is known for handling out-of-vocabulary words by splitting them into subword units.\n",
        "\n",
        "Implemented with its core algorithms in Rust. Rrust is a programming language known for its performance and memory safety, suggesting that tiktoken is designed to be computationally efficient.\n",
        "\n",
        "Used in GPT-2. This indicates that it's a relevant tool for working with models like GPT-2, particularly for preparing the text data these models process.\n",
        "Faster than the original GPT-2 tokenizer.\n",
        "\n",
        "The text notes a comparison in a separate notebook showing tiktoken was about 5x faster on sample text.\n",
        "\n",
        "In essence, tiktoken is a high-performance, open-source library for tokenizing text using the BPE algorithm, developed by OpenAI for use with their models like GPT-2."
      ],
      "metadata": {
        "id": "1SEfQR5jiWod"
      },
      "id": "1SEfQR5jiWod"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
      "metadata": {
        "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff2cd85-7cfb-4325-b390-219938589428",
      "metadata": {
        "id": "5ff2cd85-7cfb-4325-b390-219938589428",
        "outputId": "3ceae52d-af83-4ffe-da68-4e5118433a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d26a48bb-f82e-41a8-a955-a1c9cf9d50ab",
      "metadata": {
        "id": "d26a48bb-f82e-41a8-a955-a1c9cf9d50ab",
        "outputId": "74207322-044b-4011-9562-76a38e260424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ],
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a",
      "metadata": {
        "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a"
      },
      "source": [
        "- BPE tokenizers break down unknown words into subwords and individual characters:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c",
      "metadata": {
        "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp\" width=\"300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
      "metadata": {
        "id": "abbd7c0d-70f8-4386-a114-907e96c950b0"
      },
      "source": [
        "## 2.6 Data sampling with a sliding window"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0",
      "metadata": {
        "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0"
      },
      "source": [
        "- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c",
      "metadata": {
        "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "848d5ade-fd1f-46c3-9e31-1426e315c71b",
      "metadata": {
        "id": "848d5ade-fd1f-46c3-9e31-1426e315c71b",
        "outputId": "889d01b5-2184-4954-9160-f9550176307a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5145\n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cebd0657-5543-43ca-8011-2ae6bd0a5810",
      "metadata": {
        "id": "cebd0657-5543-43ca-8011-2ae6bd0a5810"
      },
      "source": [
        "- For each text chunk, we want the inputs and targets\n",
        "- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84424a7-646d-45b6-99e3-80d15fb761f2",
      "metadata": {
        "id": "e84424a7-646d-45b6-99e3-80d15fb761f2"
      },
      "outputs": [],
      "source": [
        "enc_sample = enc_text[50:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfbff852-a92f-48c8-a46d-143a0f109f40",
      "metadata": {
        "id": "dfbff852-a92f-48c8-a46d-143a0f109f40",
        "outputId": "e79ae10e-bd22-492f-aa9a-fe9b66d34870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ],
      "source": [
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815014ef-62f7-4476-a6ad-66e20e42b7c3",
      "metadata": {
        "id": "815014ef-62f7-4476-a6ad-66e20e42b7c3"
      },
      "source": [
        "- One by one, the prediction would look like as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d97b031e-ed55-409d-95f2-aeb38c6fe366",
      "metadata": {
        "id": "d97b031e-ed55-409d-95f2-aeb38c6fe366",
        "outputId": "a61ec1db-690f-446d-adf9-beeef0f3b999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57bd746-dcbf-4433-8e24-ee213a8c34a1",
      "metadata": {
        "id": "f57bd746-dcbf-4433-8e24-ee213a8c34a1",
        "outputId": "f6c117aa-14ab-4e65-dda3-2e3d0fa862fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210d2dd9-fc20-4927-8d3d-1466cf41aae1",
      "metadata": {
        "id": "210d2dd9-fc20-4927-8d3d-1466cf41aae1"
      },
      "source": [
        "- We will take care of the next-word prediction in a later chapter after we covered the attention mechanism\n",
        "- For now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1a1b47a-f646-49d1-bc70-fddf2c840796",
      "metadata": {
        "id": "a1a1b47a-f646-49d1-bc70-fddf2c840796"
      },
      "source": [
        "- Install and import PyTorch (see Appendix A for installation tips)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1770134-e7f3-4725-a679-e04c3be48cac",
      "metadata": {
        "id": "e1770134-e7f3-4725-a679-e04c3be48cac",
        "outputId": "d69d6ae5-7048-4949-8459-e8b259208700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c",
      "metadata": {
        "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c"
      },
      "source": [
        "- We use a sliding window approach, changing the position by +1:\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c",
      "metadata": {
        "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c"
      },
      "source": [
        "- Create dataset and dataloader that extract chunks from the input text dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375",
      "metadata": {
        "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e",
      "metadata": {
        "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dd68ef-59f7-45ff-ba44-e311c899ddcd",
      "metadata": {
        "id": "42dd68ef-59f7-45ff-ba44-e311c899ddcd"
      },
      "source": [
        "- Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df31d96c-6bfd-4564-a956-6192242d7579",
      "metadata": {
        "id": "df31d96c-6bfd-4564-a956-6192242d7579"
      },
      "outputs": [],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
      "metadata": {
        "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
        "outputId": "98be13e1-e84f-400e-afe6-917528b9392b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
      "metadata": {
        "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
        "outputId": "333858d8-5010-4e64-d92c-96f9c40d6bbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ],
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b006212f-de45-468d-bdee-5806216d1679",
      "metadata": {
        "id": "b006212f-de45-468d-bdee-5806216d1679"
      },
      "source": [
        "- An example using stride equal to the context length (here: 4) as shown below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3",
      "metadata": {
        "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16",
      "metadata": {
        "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16"
      },
      "source": [
        "- We can also create batched outputs\n",
        "- Note that we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
      "metadata": {
        "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
        "outputId": "6646bd04-f4c2-4f43-9e9d-4dd8e8a930b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1",
      "metadata": {
        "id": "2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1"
      },
      "source": [
        "## 2.7 Creating token embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a301068-6ab2-44ff-a915-1ba11688274f",
      "metadata": {
        "id": "1a301068-6ab2-44ff-a915-1ba11688274f"
      },
      "source": [
        "- The data is already almost ready for an LLM\n",
        "- But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
        "- Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85089aa-8671-4e5f-a2b3-ef252004ee4c",
      "metadata": {
        "id": "e85089aa-8671-4e5f-a2b3-ef252004ee4c"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e014ca-1fc5-4b90-b6fa-c2097bb92c0b",
      "metadata": {
        "id": "44e014ca-1fc5-4b90-b6fa-c2097bb92c0b"
      },
      "source": [
        "- Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a6304c-9474-4470-b85d-3991a49fa653",
      "metadata": {
        "id": "15a6304c-9474-4470-b85d-3991a49fa653"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14da6344-2c71-4837-858d-dd120005ba05",
      "metadata": {
        "id": "14da6344-2c71-4837-858d-dd120005ba05"
      },
      "source": [
        "- For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cb2cee-9aa6-4bb8-8977-c65661d16eda",
      "metadata": {
        "id": "93cb2cee-9aa6-4bb8-8977-c65661d16eda"
      },
      "outputs": [],
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ff241f6-78eb-4e4a-a55f-5b2b6196d5b0",
      "metadata": {
        "id": "4ff241f6-78eb-4e4a-a55f-5b2b6196d5b0"
      },
      "source": [
        "- This would result in a 6x3 weight matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a686eb61-e737-4351-8f1c-222913d47468",
      "metadata": {
        "id": "a686eb61-e737-4351-8f1c-222913d47468",
        "outputId": "b5cefc8e-d984-4d17-d2c2-ee1d4ba739dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fcf4f5-0801-4eb4-bb90-acce87935ac7",
      "metadata": {
        "id": "26fcf4f5-0801-4eb4-bb90-acce87935ac7"
      },
      "source": [
        "- For those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully-connected layer, which is described in the supplementary code in [./embedding_vs_matmul](../03_bonus_embedding-vs-matmul)\n",
        "- Because the embedding layer is just a more efficient implementation that is equivalent to the one-hot encoding and matrix-multiplication approach it can be seen as a neural network layer that can be optimized via backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b0d58c3-83c0-4205-aca2-9c48b19fd4a7",
      "metadata": {
        "id": "4b0d58c3-83c0-4205-aca2-9c48b19fd4a7"
      },
      "source": [
        "- To convert a token with id 3 into a 3-dimensional vector, we do the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43600ba-f287-4746-8ddf-d0f71a9023ca",
      "metadata": {
        "id": "e43600ba-f287-4746-8ddf-d0f71a9023ca",
        "outputId": "7f51656b-3f6b-4304-ce77-38c781108324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7bbf625-4f36-491d-87b4-3969efb784b0",
      "metadata": {
        "id": "a7bbf625-4f36-491d-87b4-3969efb784b0"
      },
      "source": [
        "- Note that the above is the 4th row in the `embedding_layer` weight matrix\n",
        "- To embed all four `input_ids` values above, we do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50280ead-0363-44c8-8c35-bb885d92c8b7",
      "metadata": {
        "id": "50280ead-0363-44c8-8c35-bb885d92c8b7",
        "outputId": "d94472ce-5c30-46fe-f7ed-69bd5a86efdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(input_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be97ced4-bd13-42b7-866a-4d699a17e155",
      "metadata": {
        "id": "be97ced4-bd13-42b7-866a-4d699a17e155"
      },
      "source": [
        "- An embedding layer is essentially a look-up operation:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f33c2741-bf1b-4c60-b7fd-61409d556646",
      "metadata": {
        "id": "f33c2741-bf1b-4c60-b7fd-61409d556646"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08218d9f-aa1a-4afb-a105-72ff96a54e73",
      "metadata": {
        "id": "08218d9f-aa1a-4afb-a105-72ff96a54e73"
      },
      "source": [
        "- **You may be interested in the bonus content comparing embedding layers with regular linear layers: [../03_bonus_embedding-vs-matmul](../03_bonus_embedding-vs-matmul)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c393d270-b950-4bc8-99ea-97d74f2ea0f6",
      "metadata": {
        "id": "c393d270-b950-4bc8-99ea-97d74f2ea0f6"
      },
      "source": [
        "## 2.8 Encoding word positions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24940068-1099-4698-bdc0-e798515e2902",
      "metadata": {
        "id": "24940068-1099-4698-bdc0-e798515e2902"
      },
      "source": [
        "- Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e0b14a2-f3f3-490e-b513-f262dbcf94fa",
      "metadata": {
        "id": "9e0b14a2-f3f3-490e-b513-f262dbcf94fa"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a7d7fe-38a5-46e6-8db6-b688887b0430",
      "metadata": {
        "id": "92a7d7fe-38a5-46e6-8db6-b688887b0430"
      },
      "source": [
        "- Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48de37db-d54d-45c4-ab3e-88c0783ad2e4",
      "metadata": {
        "id": "48de37db-d54d-45c4-ab3e-88c0783ad2e4"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f187f87-c1f8-4c2e-8050-350bbb972f55",
      "metadata": {
        "id": "7f187f87-c1f8-4c2e-8050-350bbb972f55"
      },
      "source": [
        "- The BytePair encoder has a vocabulary size of 50,257:\n",
        "- Suppose we want to encode the input tokens into a 256-dimensional vector representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b9e344d-03a6-4f2c-b723-67b6a20c5041",
      "metadata": {
        "id": "0b9e344d-03a6-4f2c-b723-67b6a20c5041"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2654722-24e4-4b0d-a43c-436a461eb70b",
      "metadata": {
        "id": "a2654722-24e4-4b0d-a43c-436a461eb70b"
      },
      "source": [
        "- If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\n",
        "- If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad56a263-3d2e-4d91-98bf-d0b68d3c7fc3",
      "metadata": {
        "id": "ad56a263-3d2e-4d91-98bf-d0b68d3c7fc3"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84416b60-3707-4370-bcbc-da0b62f2b64d",
      "metadata": {
        "id": "84416b60-3707-4370-bcbc-da0b62f2b64d",
        "outputId": "8cd13b4c-ff4d-437f-e293-515c988fdecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7766ec38-30d0-4128-8c31-f49f063c43d1",
      "metadata": {
        "id": "7766ec38-30d0-4128-8c31-f49f063c43d1",
        "outputId": "5461e7bb-3458-403f-a554-7bf5fbd21039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "# print(token_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2ae164-6f19-4e32-b9e5-76950fcf1c9f",
      "metadata": {
        "id": "fe2ae164-6f19-4e32-b9e5-76950fcf1c9f"
      },
      "source": [
        "- GPT-2 uses absolute position embeddings, so we just create another embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc048e20-7ac8-417e-81f5-8fe6f9a4fe07",
      "metadata": {
        "id": "cc048e20-7ac8-417e-81f5-8fe6f9a4fe07"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "\n",
        "# uncomment & execute the following line to see how the embedding layer weights look like\n",
        "# print(pos_embedding_layer.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c369a1e7-d566-4b53-b398-d6adafb44105",
      "metadata": {
        "id": "c369a1e7-d566-4b53-b398-d6adafb44105",
        "outputId": "66886943-1649-4a01-a3c0-f9660fcfce36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "# print(pos_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "870e9d9f-2935-461a-9518-6d1386b976d6",
      "metadata": {
        "id": "870e9d9f-2935-461a-9518-6d1386b976d6"
      },
      "source": [
        "- To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22fab89-526e-43c8-9035-5b7018e34288",
      "metadata": {
        "id": "b22fab89-526e-43c8-9035-5b7018e34288",
        "outputId": "5fd0772e-fe3a-4da3-b39d-5f39c1a4ba8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "# print(input_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fbda581-6f9b-476f-8ea7-d244e6a4eaec",
      "metadata": {
        "id": "1fbda581-6f9b-476f-8ea7-d244e6a4eaec"
      },
      "source": [
        "- In the initial phase of the input processing workflow, the input text is segmented into separate tokens\n",
        "- Following this segmentation, these tokens are transformed into token IDs based on a predefined vocabulary:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1bb0f7e-460d-44db-b366-096adcd84fff",
      "metadata": {
        "id": "d1bb0f7e-460d-44db-b366-096adcd84fff"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63230f2e-258f-4497-9e2e-8deee4530364",
      "metadata": {
        "id": "63230f2e-258f-4497-9e2e-8deee4530364"
      },
      "source": [
        "# Summary and takeaways"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3293a6-45a5-47cd-aa00-b23e3ca0a73f",
      "metadata": {
        "id": "8b3293a6-45a5-47cd-aa00-b23e3ca0a73f"
      },
      "source": [
        "See the [./dataloader.ipynb](./dataloader.ipynb) code notebook, which is a concise version of the data loader that we implemented in this chapter and will need for training the GPT model in upcoming chapters.\n",
        "\n",
        "See [./exercise-solutions.ipynb](./exercise-solutions.ipynb) for the exercise solutions.\n",
        "\n",
        "See the [Byte Pair Encoding (BPE) Tokenizer From Scratch](../02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb) notebook if you are interested in learning how the GPT-2 tokenizer can be implemented and trained from scratch."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}